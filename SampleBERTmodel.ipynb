from transformers import pipeline

# Load pre-trained model for question answering
qa_pipeline = pipeline("question-answering", model="/opt/bert-base-cased", tokenizer="/opt/bert-base-cased")

# Example context and question
context = "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model for natural language processing tasks."
question = "What is BERT?"

# Set max_length parameter to a larger value
max_length = 512  # You can adjust this value based on your needs

# Perform question answering with max_length
answer = qa_pipeline(question=question, context=context, max_length=max_length)

# Print the full answer
print("Answer:", answer['answer'])
